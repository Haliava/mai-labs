### 1) Теоретическая часть  
Архитектура GPT (Generative Pre-trained Transformer) основана на механизме трансформеров, предложенных в 2017 году. Её ключевая особенность — использование **многоголового самовнимания**, позволяющего модели анализировать контекстные связи между словами в последовательности.  

Основные компоненты GPT:  
- **Токенизация** — преобразование текста в числовые индексы через словарь.  
- **Эмбеддинги** — векторные представления слов, сохраняющие семантические связи.  
- **Позиционное кодирование** — добавление информации о порядке слов, так как трансформеры не имеют встроенного понимания последовательности.  
- **Декодерные блоки** — каждый блок включает слои самовнимания и полносвязные сети. Механизм внимания вычисляет веса для пар слов, определяя, насколько каждое слово влияет на другие в контексте.  
- **Предобучение** — обучение на большом корпусе текста для прогнозирования следующего слова в последовательности (задача language modeling).  

GPT исключает использование рекуррентных сетей (LSTM/GRU), полагаясь исключительно на внимание, что ускоряет обучение и улучшает качество длинных зависимостей.

---

### 2) Практическая часть  
Я реализовал текстовый генератор на основе двунаправленной LSTM, адаптировав подходы GPT для работы с ограниченными ресурсами:  

1. **Подготовка данных**:  
   - Очистил текст: привел к нижнему регистру, удалил лишние пробелы.  
   - Разбил текст на последовательности по 5 слов для предсказания следующего.  

2. **Токенизация**:  
   - Использовал `Tokenizer` из Keras с ограничением словаря до 20 000 слов.  
   - Заменил редкие слова на `<unk>` для борьбы с шумом.  

3. **Архитектура модели**:  
   - **Слой эмбеддингов** (32-мерные векторы).  
   - **Двунаправленные LSTM** (100 юнитов) с дропаутами (20%) для предотвращения переобучения.  
   - Полносвязные слои с активацией ReLU и softmax.  

4. **Обучение**:  
   - Применил `sparse_categorical_crossentropy` для работы с целочисленными метками.  
   - Использовал генератор данных (`TextGenerator`), чтобы избежать перегрузки памяти.  
   - Добавил callback для очистки памяти между эпохами.  

5. **Генерация текста**:  
   - Реализовал функцию `generate_text_words` с поддержкой температуры для управления случайностью предсказаний.  

---

### 3) Результаты  
- **Качество модели**:  
  - Финальная точность: **30.65%**.  
  - Потери (loss): **3.11**.  
  - Перплексия: **22.4** (чем ниже, тем лучше).  

- **Примеры генерации**:  
  - При температуре 0.5 текст стабилен, но повторяет шаблоны:  
    *«гарри поттер открыл дверь и <unk> может из прав и того потому что он захлопнул...»*.  
  - При температуре 1.2 вывод разнообразнее, но менее связен:  
    *«гарри поттер открыл дверь и конверт для этом один в так отец и у него тем даже...»*.  

---

### 4) Вывод  
Я научился:  
- Работать с большими текстовыми данными, оптимизируя потребление памяти через генераторы.  
- Настраивать гиперпараметры (температура, длина последовательности) для баланса между креативностью и связностью текста.  
- Бороться с переобучением с помощью дропаутов и регуляризации.  

Проблемы и улучшения:  
- Низкая точность (30%) указывает на недостаток данных или сложность модели.  
- Для улучшения результатов можно увеличить длину контекста (sequence length) или перейти к трансформерам.